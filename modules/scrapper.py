__author__ = "Lou-Poueyou Alexandre | github.com/AlexLoup33"

import requests
import os

from pathlib import Path
from bs4 import BeautifulSoup
from typing import NamedTuple
from openpyxl import Workbook
from openpyxl.worksheet._read_only import ReadOnlyWorksheet 
from openpyxl.chartsheet.chartsheet import Chartsheet
from tkinter import messagebox
from dotenv import load_dotenv
from modules.network import findNetwork, verifDNS, NetworkScrap

class EmailScrap(NamedTuple):
    email: str
    score: int

class InfoScrap(NamedTuple):
    company_name: str
    domain: str
    creationDate: str | None
    networkScrap: NetworkScrap | None
    emailPatern: str
    email : EmailScrap | None

class ActivityInfoScrap(NamedTuple):
    companyName: str
    domain: str
    creationDate: str
    departement: str
    activity : str
    NetworkScrap: NetworkScrap | None
    emailPatern: str
    email : EmailScrap | None

class Queue:
    def __init__(self, size:int):
        self.queue = []
        self.size:int = size+1
        self.pointer:int = 0

    def enqueue(self, item):
        if self.is_full():
            return
        self.queue.append(item)
        self.pointer += 1

    def dequeue(self)->str:
        if self.is_empty():
            return ""
        self.pointer -= 1
        return self.queue.pop(0)
    
    def is_empty(self):
        return self.pointer <= 0
    
    def is_full(self):
        return self.pointer == self.size
    
    def display(self):
        for item in self.queue:
            print(item)

envPath = Path(__file__).parent.parent.joinpath("venv") /".env"
envVar = load_dotenv(envPath)
api_key = os.getenv("HUNTER_API_KEY")

"""
Const who use Path lib to get the relative path of the directory 'save' and 'savetab' in the project for 
saving the csv file and the tabsheet file
"""
csvPath = Path(__file__).parent.parent.joinpath("save")
tabPath = Path(__file__).parent.parent.joinpath("savetab")

"""
Checking for the existence of the directories 'save' and 'savetab' and creating them if they don't exist
save is used to save the csv file generated by the program and who contain the basics informations of the companies (name, domain)
savetab is used to save the tabsheet file generated by the program and who contain all the 
informations findable of the companies and contain in the class InfoScrap (company name, domain, email, email score, contact page, 
facebook, twitter, linkedin)
Note: the tabsheet can not contain all the informations of the companies, because some companies don't have a contact page, or 
hide somes informations on their website
"""
if not csvPath.exists():
    csvPath.mkdir()
if not tabPath.exists():
    tabPath.mkdir()

def scrapRevenue(url:str, number:int, csvFileName:str, tabName:str, tabFileName:str)->None:
    print(f"Started to scrap at url {url}")
    """
    Get the html element of the page given, and parse it with BeautifulSoup (for the version, we will use only verif.com pages)
    At least, add the option to give other params like a list of company names, or a list of company domains.
    """
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    """
    Get the value in the span with class "MuiTypography-root MuiTypography-titleDesktopH4 css-1ltityp" who contain the number
    of companies in the region and compare with the number of companies the user wants.
    Also check if the element exist on the page by shuting down the function it if it's None because it's mean the page isn't
    a verif.com page or a good one to scrap
    """
    TotalCompaniesHtmlElement = soup.find_all('span', class_="MuiTypography-root MuiTypography-titleDesktopH4 css-1ltityp")
    assert TotalCompaniesHtmlElement is not None


    TotalCompanies:int = int(extractFloatFromString(TotalCompaniesHtmlElement[0].text.strip()))
    size:int = (TotalCompanies if number > TotalCompanies else number)

    """
    Initialize the queue with the size of the number of companies desired
    Note: The queue will not necessary be full, if the verif page of the company doesn't have a domain, 
    the program will not add it in the queue
    """
    companyNameQueue = Queue(size)
    companyDomainQueue = Queue(size)
    companyCreationDateQueue = Queue(size)
    

    """
    Those variables are used to go through the companies on the page, the index is used to get the company at the index
    and the pageCounter is used to know on which page we are
    """
    index = 0
    pageCounter = 0

    index, pageCounter = fillQueue(companyNameQueue, companyDomainQueue, companyCreationDateQueue, None, None, url, index, pageCounter, size)
    
    """
    Now we have all the informations needed to scrap the companies, we will start to search for the emails of the companies
    and the social network and store it in the class NetworkScrap and EmailScrap and then store it in the class InfoScrap
    """

    """
    Open and initialize the workbook and the worksheet of the workbook
    will write each row with the informations of the companies which time i dequeue the company name and the domain
    """
    wb = Workbook()
    ws = wb.active

    assert ws is not None
    ws.title = tabName

    assert not isinstance(ws, (ReadOnlyWorksheet, Chartsheet))
    ws.append([f"Nom de l'entreprise", "Nom de Domain", "Date de création", "Page de Contact", "Facebook", "Twitter", "Linkedin", "Email", "Score du mail", "Pattern de l'email, {url},{index}, {pageCounter}"])

    """
    Open and initialize the csv file with the same informations as the tabsheet
    I also use a csv file to store the informations of the companies, because it's easier to read and to use
    but also can be modified with a text editor
    """
    csvFile = open(f"{csvPath}/{csvFileName}.csv", "w", encoding="utf-8")
    csvFile.write(f"Nom de l'entreprise, Nom de Domain, Date de création, Page de Contact, Facebook, Twitter, Linkedin, Email, Score du mail, Pattern de l'email, {url}, {index}, {pageCounter}\n")

    data:"list[InfoScrap]" = [] 
    while(not companyNameQueue.is_empty()):
        company_name:str = companyNameQueue.dequeue()
        domain:str = companyDomainQueue.dequeue()
        creationDate:str = companyCreationDateQueue.dequeue()
        networkScrap = companyNetwork(domain)


        """
        Email scrap section, we will search for email with the hunter.io API, 
        the API will return the email of the company, a score and a email pattern
        """
        response = requests.get(f"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={api_key}")
        responseData = response.json()


        """
        Don't process the email & score with the pattern if the API doesn't find any email but find a pattern
        """
        try:
            pattern = responseData['data']['pattern']
        except (IndexError, KeyError):
            pattern = "Non disponible"

        try:
            email = responseData['data']['emails'][0]['value']
            score = int(responseData['data']['emails'][0]['confidence'])
        except (IndexError, KeyError):
            emailScrap = None
        else: emailScrap = EmailScrap(email, score)


        """
        The API of Hunter.io can find the email and the pattern of the email, 
        but also can find the social network of the company
        """
        if networkScrap is not None:
            if networkScrap.twitter is None:
                try : networkScrap.twitter = responseData['data']['twitter']
                except (IndexError, KeyError): pass
            
            if networkScrap.facebook is None:
                try : networkScrap.facebook = responseData['data']['facebook']
                except (IndexError, KeyError): pass
            
            if networkScrap.linkedin is None:
                try : networkScrap.linkedin = responseData['data']['linkedin']
                except (IndexError, KeyError): pass
        

        if not pattern and not emailScrap:
            print(f"Email not found for {company_name}")

        data.append(InfoScrap(company_name, domain, creationDate, networkScrap, pattern, emailScrap))
    
    for row in data:
        email = row.email
        if email is None:
            mail = "Non disponible"
            score = "Score non disponible"
        else: 
            mail = email.email
            score = email.score 

        network = row.networkScrap
        if network is None:
            contactPage = "Non disponible"
            facebook = "Non disponible"
            twitter = "Non disponible"
            linkedin = "Non disponible"
        else: 
            if network.contactPage is None: contactPage = "Non disponible"
            else: contactPage = network.contactPage
            if network.facebook is None: facebook = "Non disponible"
            else: facebook = network.facebook
            if network.twitter is None: twitter = "Non disponible"
            else: twitter = network.twitter
            if network.linkedin is None: linkedin = "Non disponible"
            else: linkedin = network.linkedin
        

        ws.append([row.company_name, row.domain, row.creationDate, contactPage, facebook, twitter, linkedin, mail, score, row.emailPatern])
        csvFile.write(f"{row.company_name}, {row.domain}, {row.creationDate},{contactPage}, {facebook}, {twitter}, {linkedin}, {mail}, {score}, {row.emailPatern}\n")
    
    wb.save(f"{tabPath}/{tabFileName}.xlsx")
    csvFile.close()

    messagebox.showinfo("Information", "Le scrap des sociétés est effectué avec succès ! Vous pouvez retrouver les informations dans le fichier companies.xlsx et companies.csv dans le dossier savetab et save respectivement.")

def scrapActivity(url:str, number:int, csvFileName:str, tabName:str, tabFileName:str)->None:
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    TotalCompaniesHtmlElement = soup.find_all('span', class_="MuiTypography-root MuiTypography-titleDesktopH4 css-1ltityp")
    assert TotalCompaniesHtmlElement is not None


    TotalCompanies:int = int(extractFloatFromString(TotalCompaniesHtmlElement[0].text.strip()))
    size:int = (TotalCompanies if number > TotalCompanies else number)

    companyNameQueue = Queue(size)
    companyDomainQueue = Queue(size)
    companyCreationDateQueue = Queue(size)
    companyDepartementQueue = Queue(size)
    companyActivityQueue = Queue(size)

    index = 0
    pageCounter = 0

    index, pageCounter = fillQueue(companyNameQueue, companyDomainQueue, companyCreationDateQueue, companyDepartementQueue , companyActivityQueue, url, index, pageCounter, size)

    wb = Workbook()
    ws = wb.active

    assert ws is not None
    ws.title = tabName
    
    assert not isinstance(ws, (ReadOnlyWorksheet, Chartsheet))
    ws.append(["Nom de l'entreprise", "Nom de Domain", "Date de création", "Département", "Activité", "Page de Contact", "Facebook", "Twitter", "Linkedin", "Email", "Score du mail", "Pattern de l'email", f"{url}, {index}, {pageCounter}"])

    csvFile = open(f"{csvPath}/{csvFileName}.csv", "w", encoding="utf-8")
    csvFile.write(f"Nom de l'entreprise, Nom de Domain, Date de création, Département, Activité, Page de Contact, Facebook, Twitter, Linkedin, Email, Score du mail, Pattern de l'email, {url}, {index}, {pageCounter}\n")

    data:"list[ActivityInfoScrap]" = []
    while(not companyNameQueue.is_empty()):
        company_name:str = companyNameQueue.dequeue()
        domain:str = companyDomainQueue.dequeue()
        creationDate:str = companyCreationDateQueue.dequeue()
        departement:str = companyDepartementQueue.dequeue()
        activity:str = companyActivityQueue.dequeue()
        networkScrap = companyNetwork(domain)

        response = requests.get(f"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={api_key}")
        responseData = response.json()

        try:
            pattern = responseData['data']['pattern']
        except (IndexError, KeyError):
            pattern = "Non disponible"

        try:
            email = responseData['data']['emails'][0]['value']
            score = int(responseData['data']['emails'][0]['confidence'])
        except (IndexError, KeyError):
            emailScrap = None
        else: emailScrap = EmailScrap(email, score)

        if networkScrap is not None:
            if networkScrap.twitter is None:
                try : networkScrap.twitter = responseData['data']['twitter']
                except (IndexError, KeyError): pass
            
            if networkScrap.facebook is None:
                try : networkScrap.facebook = responseData['data']['facebook']
                except (IndexError, KeyError): pass
            
            if networkScrap.linkedin is None:
                try : networkScrap.linkedin = responseData['data']['linkedin']
                except (IndexError, KeyError): pass

        if not pattern and not emailScrap:
            print(f"Email not found for {company_name}")

        data.append(ActivityInfoScrap(company_name, domain, creationDate, departement, activity, networkScrap, pattern, emailScrap))

    for row in data:
        email = row.email
        if email is None:
            mail = "Non disponible"
            score = "Score non disponible"
        else: 
            mail = email.email
            score = email.score 

        network = row.NetworkScrap
        if network is None:
            contactPage = "Non disponible"
            facebook = "Non disponible"
            twitter = "Non disponible"
            linkedin = "Non disponible"
        else: 
            if network.contactPage is None: contactPage = "Non disponible"
            else: contactPage = network.contactPage
            if network.facebook is None: facebook = "Non disponible"
            else: facebook = network.facebook
            if network.twitter is None: twitter = "Non disponible"
            else: twitter = network.twitter
            if network.linkedin is None: linkedin = "Non disponible"
            else: linkedin = network.linkedin
        

        ws.append([row.companyName, row.domain, row.creationDate, row.departement, row.activity, contactPage, facebook, twitter, linkedin, mail, score, row.emailPatern])
        csvFile.write(f"{row.companyName}, {row.domain}, {row.creationDate},{row.departement}, {row.activity}, {contactPage}, {facebook}, {twitter}, {linkedin}, {mail}, {score}, {row.emailPatern}\n")
    
    wb.save(f"{tabPath}/{tabFileName}.xlsx")
    csvFile.close()
    messagebox.showinfo("Information", "Le scrap des sociétés est effectué avec succès ! Vous pouvez retrouver les informations dans le fichier companies.xlsx et companies.csv dans le dossier savetab et save respectivement.")


def scrapFromFile(filename: str, number: int, typeSearch: bool)->None:
    """
    Get the first line of the file to get the url, the number of companies scrap 
    and the number of the page
    """
    with open(f"save/{filename}", "r") as file:
        lines = file.readlines()
        firstLine = lines[0].split(", ")
        if typeSearch:
            url = firstLine[12]
            index = int(firstLine[13])
            pageCounter = int(firstLine[14])
        else: 
            url = firstLine[10]
            index = int(firstLine[11])
            pageCounter = int(firstLine[12])

    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    TotalCompaniesHtmlElement = soup.find_all('span', 
                                            class_="MuiTypography-root MuiTypography-titleDesktopH4 css-1ltityp")
    assert TotalCompaniesHtmlElement is not None

    TotalCompanies:int = int(extractFloatFromString(TotalCompaniesHtmlElement[0].text.strip()))
    size:int = (TotalCompanies if number > TotalCompanies else number)

    companyName = Queue(size)
    companyDomain = Queue(size)
    companyCreationDate = Queue(size)
    companyDepartement = None
    companyActivity = None
    
    if typeSearch:
        companyDepartement = Queue(size)
        companyActivity = Queue(size)

    print("before fillQueue")

    newIndex, newPageCounter = fillQueue(companyName, companyDomain, companyCreationDate, 
                                         companyDepartement, companyActivity, url, 
                                         index, pageCounter, TotalCompanies, None)

    print("after fillQueue")

    companyName.display()
    companyDomain.display()
    companyCreationDate.display()


    if typeSearch and companyDepartement is not None and companyActivity is not None:
        companyDepartement.display()
        companyActivity.display()

    data = []

    """
    For the search by a file, it only use csv file because the only type of input is a csv file
    """
    csvFile = open(f"{csvPath}/{filename}", "a+", encoding="utf-8")
    while not companyName.is_empty():
        company_name = companyName.dequeue()
        domain = companyDomain.dequeue()
        creationDate = companyCreationDate.dequeue()

        if typeSearch and companyDepartement is not None and companyActivity is not None:
            departement = companyDepartement.dequeue()
            activity = companyActivity.dequeue()

        networkScrap = companyNetwork(domain)

        response = requests.get(f"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={api_key}")
        responseData = response.json()

        try:
            pattern = responseData['data']['pattern']
        except (IndexError, KeyError):
            pattern = "Non disponible"

        try:
            email = responseData['data']['emails'][0]['value']
            score = int(responseData['data']['emails'][0]['confidence'])
        except (IndexError, KeyError):
            emailScrap = None
        else: emailScrap = EmailScrap(email, score)

        if networkScrap is not None:
            if networkScrap.twitter is None:
                try : networkScrap.twitter = responseData['data']['twitter']
                except (IndexError, KeyError): pass
            
            if networkScrap.facebook is None:
                try : networkScrap.facebook = responseData['data']['facebook']
                except (IndexError, KeyError): pass
            
            if networkScrap.linkedin is None:
                try : networkScrap.linkedin = responseData['data']['linkedin']
                except (IndexError, KeyError): pass

        if not pattern and not emailScrap:
            print(f"Email not found for {company_name}")

        if typeSearch:
            data.append(ActivityInfoScrap(company_name, domain, creationDate, departement, activity, networkScrap, pattern, emailScrap))
        else: 
            data.append(InfoScrap(company_name, domain, creationDate, networkScrap, pattern, emailScrap))
    
    for row in data:
        email = row.email
        if email is None:
            mail = "Non disponible"
            score = "Score non disponible"
        else: 
            mail = email.email
            score = email.score 

        if typeSearch:
            network = row.NetworkScrap
        else : 
            network = row.networkScrap
        if network is None:
            contactPage = "Non disponible"
            facebook = "Non disponible"
            twitter = "Non disponible"
            linkedin = "Non disponible"
        else: 
            if network.contactPage is None: contactPage = "Non disponible"
            else: contactPage = network.contactPage
            if network.facebook is None: facebook = "Non disponible"
            else: facebook = network.facebook
            if network.twitter is None: twitter = "Non disponible"
            else: twitter = network.twitter
            if network.linkedin is None: linkedin = "Non disponible"
            else: linkedin = network.linkedin
        
        if not typeSearch: 
            lines.append(f"{row.company_name}, {row.domain}, {row.creationDate},{contactPage}, {facebook}, {twitter}, {linkedin}, {mail}, {score}, {row.emailPatern}\n")
        else: 
            lines.append(f"{row.companyName}, {row.domain}, {row.creationDate},{row.departement}, {row.activity}, {contactPage}, {facebook}, {twitter}, {linkedin}, {mail}, {score}, {row.emailPatern}\n")
    csvFile.close()

    if typeSearch:
        lines[0] = f"Nom de l'entreprise, Nom de Domain, Date de création, Département, Activité, Page de Contact, Facebook, Twitter, Linkedin, Email, Score du mail, Pattern de l'email, {url}, {newIndex}, {newPageCounter}\n"
    else:
        lines[0] = f"Nom de l'entreprise, Nom de Domain, Date de création, Page de Contact, Facebook, Twitter, Linkedin, Email, Score du mail, Pattern de l'email, {url}, {newIndex}, {newPageCounter}\n"

    with open(f"save/{filename}", "w") as file:
        file.writelines(lines)
    file.close()

    messagebox.showinfo("Information", "Le scrap des sociétés est effectué avec succès ! Vous pouvez retrouver les informations dans le fichier companies.csv dans le dossier save.")
    return

def extractFloatFromString(string:str)->float:
    """
    Extract the float from a string, the string must have a number in it
    Allow then to get the number of companies in the region
    """
    return float(''.join(filter(lambda x: x.isdigit() or x == '.', string)))


def fillQueue(nameQueue: Queue, domainQueue: Queue, creationDateQueue: Queue, departementQueue: "Queue|None", activityQueue: "Queue|None",
              url:str, index:int, pageCounter:int, maxCompanyCount:int, comp=None)->"tuple[int, int]":
    """
    Fill the both queue while they aren't full, must fill the queue only 
    if the company isn't already in the queue and if the company has a domain
    """
    while not nameQueue.is_full():
        """
        Check if we reach the max number of companies available on the page, if it's the case, break the loop
        and exit the function to avoid an error and process with the companies already in the queue
        """
        if pageCounter*index >= maxCompanyCount:
            break

        if comp is None:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            companies = soup.find_all('tr', class_="MuiBox-root css-1vaqj3c")
            comp = "companies"

        if index == 0:
            pageCounter += 1
            url = createPageLink(url, pageCounter)
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            companies = soup.find_all('tr', class_="MuiBox-root css-1vaqj3c")
        
        """
        Get the relative link of the company page on verif.com and combine it with the base url to get the full link
        """
        companyLink = companies[index].find('a')['href']
        fullLink = "https://www.verif.com" + companyLink

        """
        Check in the page if the company has a domain, if it's the case, add the company name and the domain in the queue
        else pass to the next company
        We only store the company who has a public domain to avoid to search for a contact page 
        """
        response = requests.get(fullLink)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        domainHtmlElement = soup.find('span', class_="MuiTypography-root MuiTypography-bodySmallMedium css-1ymqwc8")
        creationDataHtmlElement = soup.find_all('span', class_="MuiTypography-root MuiTypography-bodyDefaultMedium css-1185ny9")


        if domainHtmlElement is None:
            index = (index+1)%250
            continue
        domain = domainHtmlElement.text

        domain:str = domainHtmlElement.find('a').text # type: ignore

        creationDate = "Non disponible"
        for el in creationDataHtmlElement: #Only take the format "dd/mm/yyyy"
            if "/" in el.text:
                creationDate = el.text
                break

        if not "http://" in domain: Tmpdomain = "http://" + domain
        elif not "https://" in domain : Tmpdomain = "https://"+ domain

        if domain in domainQueue.queue or not verifDNS(domain, Tmpdomain):
            index = (index+1)%250
            continue
        
        if departementQueue is not None and activityQueue is not None:
            companyName = companies[index].find_all('h3')[0].text
            departement = companies[index].find_all('h3')[1].text
            activity = companies[index].find_all('h3')[2].text
        else : 
            companyName = companies[index].find('h3').text.strip()
        """
        The Company name is in the h3 tag in the company page, so we need to get the text in the tag and add it in the queue
        When we add the company name in the queue, we need to add the domain in the domain queue too and then we can pass to the next company
        """
        nameQueue.enqueue(companyName)
        domainQueue.enqueue(domain)
        creationDateQueue.enqueue(creationDate)
        if departementQueue is not None and activityQueue is not None:
            departementQueue.enqueue(departement)
            activityQueue.enqueue(activity)

        index = (index+1)%250 #The number max of companies on a verif page
    return index, pageCounter

def createPageLink(defaultUrl:str, pageCounter:int)->str:
    """
    The first page is the only one who doesn't have a "-page{pageNumber}/" at the end of the url
    Exemple Format : https://www.verif.com/top/revenue-r0/france-rcoun/region-rreg/department-rdep/
    """
    if pageCounter == 1:
        return defaultUrl
    """
    If the isn't the first page of the search, the url isn't same, so we need to add "-page{pageNumber}/"
    at the end of the url, where pageNumber is the number of the page
    Exemple Format with a number of page : https://www.verif.com/top/revenue-r0/france-rcoun/region-rreg/department-rdep-pageN/
    with N the number of the page

    Note : defaultUrl[:-1] is used to remove the last character of the url, in this case, the last "/" to add "-page{pageNumber}/"
    """
    return defaultUrl[:-1] + f"-page{pageCounter}/"


"""
Call the functions in the files network.py and will search the contact page, facebook, twitter and linkedin of the given company
Possibility to add more social network in the future (if needed)
Note : the program cannot find all the informations of the company, because some companies don't have a contact page, or
hide some informations on their website.
Some personal search can be needed to find the informations of the company,if the program doesn't find them

TO-DO: Find a solution for page like "https://cdiscount.com" where i can't scrap any informations
DONE : Use selenium library to skip the page with a cookie banner or captcha
"""
def companyNetwork(domain:str)->"NetworkScrap|None":
    return findNetwork(domain)