import requests
import tkinter as tk
import winsound

from pathlib import Path
from bs4 import BeautifulSoup
from typing import NamedTuple
from openpyxl import Workbook
from openpyxl.worksheet._read_only import ReadOnlyWorksheet 
from openpyxl.chartsheet.chartsheet import Chartsheet
from tkinter import messagebox
from network import *

class EmailScrap(NamedTuple):
    email: str
    score: int

class InfoScrap(NamedTuple):
    company_name: str
    domain: str
    networkScrap: NetworkScrap | None
    emailPatern: str
    email : EmailScrap | None

class Queue:
    def __init__(self, size:int):
        self.queue = []
        self.size:int = size+1
        self.pointer:int = 0

    def enqueue(self, item):
        if self.is_full():
            return
        self.queue.append(item)
        self.pointer += 1

    def dequeue(self)->str:
        if self.is_empty():
            return ""
        self.pointer -= 1
        return self.queue.pop(0)
    
    def is_empty(self):
        return self.pointer <= 0
    
    def is_full(self):
        return self.pointer == self.size
    
    def display(self):
        for item in self.queue:
            print(item)

companies_names_scrap: "list[str]" = [] #will contain all the companies names on verif.com
companies_link_scrap: "list[str]" = [] #will contain all the companies link on verif.com
companies_domains_scrap: "list[str]" = [] #will contain all the companies domain on verif.com
companies_emails_scrap: "list[EmailScrap | None]" = [] #will contain all the companies emails

api_key = "d4d29913091c9954368733ea3f29bbced2a8c63e"

"""
Const who use Path lib to get the relative path of the directory 'save' and 'savetab' in the project for 
saving the csv file and the tabsheet file
"""
csvPath = Path(__file__).parent.joinpath("save")
tabPath = Path(__file__).parent.joinpath("savetab")

"""
Checking for the existence of the directories 'save' and 'savetab' and creating them if they don't exist
save is used to save the csv file generated by the program and who contain the basics informations of the companies (name, domain)
savetab is used to save the tabsheet file generated by the program and who contain all the 
informations findable of the companies and contain in the class InfoScrap (company name, domain, email, email score, contact page, 
facebook, twitter, linkedin)
Note: the tabsheet can not contain all the informations of the companies, because some companies don't have a contact page, or 
hide somes informations on their website
"""
if not csvPath.exists():
    csvPath.mkdir()
if not tabPath.exists():
    tabPath.mkdir()

def scrap(url:str, number:int)->None:
    """
    Get the html element of the page given, and parse it with BeautifulSoup (for the version, we will use only verif.com pages)
    At least, add the option to give other params like a list of company names, or a list of company domains.
    """
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')

    """
    Get the value in the span with class "MuiTypography-root MuiTypography-titleDesktopH4 css-1ltityp" who contain the number
    of companies in the region and compare with the number of companies the user wants.
    Also check if the element exist on the page by shuting down the function it if it's None because it's mean the page isn't
    a verif.com page or a good one to scrap
    """
    TotalCompaniesHtmlElement = soup.find_all('span', class_="MuiTypography-root MuiTypography-titleDesktopH4 css-1ltityp")
    assert TotalCompaniesHtmlElement is not None


    TotalCompanies:int = int(extractFloatFromString(TotalCompaniesHtmlElement[0].text.strip()))
    size:int = (TotalCompanies if number > TotalCompanies else number)

    """
    Initialize the queue with the size of the number of companies desired
    Note: The queue will not necessary be full, if the verif page of the company doesn't have a domain, 
    the program will not add it in the queue
    """
    companyNameQueue = Queue(size)
    companyDomainQueue = Queue(size)

    """
    Those variables are used to go through the companies on the page, the index is used to get the company at the index
    and the pageCounter is used to know on which page we are
    """
    index = 0
    pageCounter = 0

    fillQueue(companyNameQueue, companyDomainQueue, url, index, pageCounter, size)

    #companyNameQueue.display()
    #companyDomainQueue.display()
    
    """
    Now we have all the informations needed to scrap the companies, we will start to search for the emails of the companies
    and the social network and store it in the class NetworkScrap and EmailScrap and then store it in the class InfoScrap
    """

    """
    Open and initialize the workbook and the worksheet of the workbook
    will write each row with the informations of the companies which time i dequeue the company name and the domain
    """
    wb = Workbook()
    ws = wb.active

    assert ws is not None
    ws.title = "Companies"

    assert not isinstance(ws, (ReadOnlyWorksheet, Chartsheet))
    ws.append(["Nom de l'entreprise", "Nom de Domain", "Page de Contact", "Facebook", "Twitter", "Linkedin", "Email", "Score du mail", "Pattern de l'email"])

    """
    Open and initialize the csv file with the same informations as the tabsheet
    I also use a csv file to store the informations of the companies, because it's easier to read and to use
    but also can be modified with a text editor
    """
    csvFile = open(f"{csvPath}/companies.csv", "w", encoding="utf-8")
    csvFile.write("Nom de l'entreprise, Nom de Domain, Page de Contact, Facebook, Twitter, Linkedin, Email, Score du mail, Pattern de l'email\n")

    data:"list[InfoScrap]" = [] 
    while(not companyNameQueue.is_empty()):
        company_name:str = companyNameQueue.dequeue()
        domain:str = companyDomainQueue.dequeue()
        networkScrap = companyNetwork(domain)


        """
        Email scrap section, we will search for email with the hunter.io API, the API will return the email of the company, a score and a email pattern
        """

        response = requests.get(f"https://api.hunter.io/v2/domain-search?domain={domain}&api_key={api_key}")
        responseData = response.json()

        """
        Don't process the email & score with the pattern if the API doesn't find any email but find a pattern
        """
        try:
            pattern = responseData['data']['pattern']
        except (IndexError, KeyError):
            pattern = "Non disponible"

        try:
            email = responseData['data']['emails'][0]['value']
            score = 0
        except (IndexError, KeyError):
            emailScrap = None
        else: emailScrap = EmailScrap(email, score)

        if not pattern and not emailScrap:
            print(f"Email not found for {company_name}")

        data.append(InfoScrap(company_name, domain, networkScrap, pattern, emailScrap))
    
    for row in data:
        email = row.email
        if email is None:
            mail = "Non disponible"
            score = "Score non disponible"
        else: 
            mail = email.email
            score = email.score 

        network = row.networkScrap
        if network is None:
            contactPage = "Non disponible"
            facebook = "Non disponible"
            twitter = "Non disponible"
            linkedin = "Non disponible"
        else: 
            if network.contactPage is None: contactPage = "Non disponible"
            else: contactPage = network.contactPage
            if network.facebook is None: facebook = "Non disponible"
            else: facebook = network.facebook
            if network.twitter is None: twitter = "Non disponible"
            else: twitter = network.twitter
            if network.linkedin is None: linkedin = "Non disponible"
            else: linkedin = network.linkedin
        

        ws.append([row.company_name, row.domain, contactPage, facebook, twitter, linkedin, mail, score, row.emailPatern])
        csvFile.write(f"{row.company_name}, {row.domain}, {contactPage}, {facebook}, {twitter}, {linkedin}, {mail}, {score}, {row.emailPatern}\n")
    
    wb.save(f"{tabPath}/companies.xlsx")
    csvFile.close()

    messagebox.showinfo("Information", "Le scrap des sociétés est effectué avec succès ! Vous pouvez retrouver les informations dans le fichier companies.xlsx et companies.csv dans le dossier savetab et save respectivement.")
        

    pass #temporary end of the function

def extractFloatFromString(string:str)->float:
    """
    Extract the float from a string, the string must have a number in it
    Allow then to get the number of companies in the region
    """
    return float(''.join(filter(lambda x: x.isdigit() or x == '.', string)))

def fillQueue(nameQueue: Queue, domainQueue: Queue, url:str, index:int, pageCounter:int, maxCompanyCount:int)->None:
    """
    Fill the both queue while they aren't full, must fill the queue only 
    if the company isn't already in the queue and if the company has a domain
    """
    while not nameQueue.is_full():
        """
        Check if we reach the max number of companies available on the page, if it's the case, break the loop
        and exit the function to avoid an error and process with the companies already in the queue
        """
        if pageCounter*index >= maxCompanyCount:
            break

        if index == 0:
            pageCounter += 1
            url = createPageLink(url, pageCounter)
            response = requests.get(url)
            soup = BeautifulSoup(response.text, 'html.parser')
            companies = soup.find_all('tr', class_="MuiBox-root css-1vaqj3c")
        
        """
        Get the relative link of the company page on verif.com and combine it with the base url to get the full link
        """
        companyLink = companies[index].find('a')['href']
        fullLink = "https://www.verif.com" + companyLink

        """
        Check in the page if the company has a domain, if it's the case, add the company name and the domain in the queue
        else pass to the next company
        We only store the company who has a public domain to avoid to search for a contact page 
        """
        response = requests.get(fullLink)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        domainHtmlElement = soup.find('span', class_="MuiTypography-root MuiTypography-bodySmallMedium css-1ymqwc8")

        
        if domainHtmlElement is None:
            index = (index+1)%250
            continue
        domain = domainHtmlElement.text

        domain:str = domainHtmlElement.find('a').text # type: ignore
        
        if not "http://" in domain: Tmpdomain = "http://" + domain
        elif not "https://" in domain : Tmpdomain = "https://"+ domain

        if domain in domainQueue.queue or not verifDNS(domain, Tmpdomain):
            index = (index+1)%250
            continue
            

        print(domain + " who passed the verifDNS")
        """
        The Company name is in the h3 tag in the company page, so we need to get the text in the tag and add it in the queue
        When we add the company name in the queue, we need to add the domain in the domain queue too and then we can pass to the next company
        """
        nameQueue.enqueue(companies[index].find('h3').text.strip())
        domainQueue.enqueue(domain)

        index = (index+1)%250 #The number max of companies on a verif page
    return



def createPageLink(defaultUrl:str, pageCounter:int)->str:
    """
    The first page is the only one who doesn't have a "-page{pageNumber}/" at the end of the url
    Exemple Format : https://www.verif.com/top/revenue-r0/france-rcoun/region-rreg/department-rdep/
    """
    if pageCounter == 1:
        return defaultUrl
    """
    If the isn't the first page of the search, the url isn't same, so we need to add "-page{pageNumber}/"
    at the end of the url, where pageNumber is the number of the page
    Exemple Format with a number of page : https://www.verif.com/top/revenue-r0/france-rcoun/region-rreg/department-rdep-pageN/
    with N the number of the page

    Note : defaultUrl[:-1] is used to remove the last character of the url, in this case, the last "/" to add "-page{pageNumber}/"
    """
    return defaultUrl[:-1] + f"-page{pageCounter}/"


"""
Call the functions in the files network.py and will search the contact page, facebook, twitter and linkedin of the given company
Possibility to add more social network in the future (if needed)
Note : the program cannot find all the informations of the company, because some companies don't have a contact page, or
hide some informations on their website.
Some personal search can be needed to find the informations of the company,if the program doesn't find them

TO-DO: Find a solution for page like "https://cdiscount.com" where i can't scrap any informations
"""
def companyNetwork(domain:str)->"NetworkScrap|None":
    return findNetwork(domain)


def main():
    #Create a graphic interface
    root = tk.Tk()
    root.title("Saine Mail Scrapper")
    root.geometry("500x500")
    root.resizable(True, True)

    #Create a label
    label = tk.Label(root, text="Saine Mail Scrapper", font=("Arial", 15))
    label.pack()

    #Create a margin
    margin = tk.Label(root, text="")
    margin.pack()

    #Create an input for the url
    urlLabel = tk.Label(root, text="Url")
    urlLabel.pack()
    url = tk.Entry(root, width=50)
    url.pack()

    #Create a slider for the number of companies who start from 1 to the number of companies
    sliderLabel = tk.Label(root, text="Number of companies")
    sliderLabel.pack()
    slider = tk.Entry(root, width=50)
    slider.pack()



    #Create an input for the name of the file csv
    csvLabel = tk.Label(root, text="Name of the csv file")
    csvLabel.pack()
    name_csv = tk.Entry(root, width=50)
    name_csv.pack()

    #Create an input for the name of the tabsheet
    tabLabel = tk.Label(root, text="Name of the tabsheet")
    tabLabel.pack()
    name_tab = tk.Entry(root, width=50)
    name_tab.pack()

    #Create an input for the name of the file tabsheet
    fileLabel = tk.Label(root, text="Name of the file tabsheet")
    fileLabel.pack()
    name_file = tk.Entry(root, width=50)
    name_file.pack()

    def button_func():
        try:
            number = int(float(slider.get()))
        except ValueError:
            messagebox.showerror("Error", "Le nombre de sociétés doit être un nombre")
            return
        return scrap(url.get(), number)#, name_csv.get(), name_tab.get(), name_file.get())

    #Create a button that scrap the companies, make a label appear when the function is done 
    button = tk.Button(root, text="Scrap", command=button_func)
    button.pack(pady=10)

    root.mainloop()

if __name__ == "__main__":
    main()